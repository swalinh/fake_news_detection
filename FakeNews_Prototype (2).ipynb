{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection\n",
    "\n",
    "1. Pre-processing\n",
    "2. Feature Extraction\n",
    "3. Classification Model\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-plot in c:\\program64\\python\\python\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\program64\\python\\python\\lib\\site-packages (from scikit-plot) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\program64\\python\\python\\lib\\site-packages (from scikit-plot) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\program64\\python\\python\\lib\\site-packages (from scikit-plot) (3.5.1)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\program64\\python\\python\\lib\\site-packages (from scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (4.28.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program64\\python\\python\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program64\\python\\python\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program64\\python\\python\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing The most fundamental libraries\n",
    "\n",
    "!pip install scikit-plot\n",
    "# !pip install wordcloud\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "# for pre-processing dataset\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "# for Splitting our dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for building classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "# for evaluation our model\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# for plotting our confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "\n",
    "#specify english stop words only\n",
    "nltk.download('stopwords')\n",
    "stops= stopwords.words('english') \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# append rt for stop word dictionary\n",
    "stops.append(\"rt\") \n",
    "\n",
    "#Create stemmer obejct\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanning Our dataset by removing unwanted Characters, Non Letters and Punctuation\n",
    "def cleanText(csv_file):\n",
    "    # Reading our dataset as pandas dataframe\n",
    "    data = pd.read_csv(csv_file)\n",
    "    # dropping the id, title and author column\n",
    "    data = data.drop(columns=['id','author']) \n",
    "    # droping all null values in our data\n",
    "    data = data.dropna() \n",
    "    data = data.reset_index(drop = True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Cleanning our text and converting it to lower case, delete stopwords, Stemming and remove punctuation\n",
    "def stem_tokenize(data):\n",
    "\n",
    "    # Frist converting all letters to lower case\n",
    "    data= data.lower()\n",
    "\n",
    "    # removing unwanted digits ,special chracters from the text\n",
    "    data= ' '.join(re.sub(\"(@[A-Za-z0-9]+)\", \" \", data).split())\n",
    "    data= ' '.join(re.sub(\"^@?(\\w){1,15}$\", \" \", data).split())\n",
    "    data= ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", data).split())\n",
    "\n",
    "    # removing stopwards and numbers from STRING library\n",
    "    table= str.maketrans('', '', string.punctuation+string.digits)\n",
    "    data = data.translate(table)\n",
    "  \n",
    "    # Split Sentence as tokens words \n",
    "    token = word_tokenize(data)\n",
    "  \n",
    "    # converting words to their root forms by STEMMING THE WORDS \n",
    "    stem = [porter.stem(word) for word in token] \n",
    "  \n",
    "    # remove stopwords from our text\n",
    "    words = [word for word in stem if not word in stops]\n",
    "    data  = ' '.join(words)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Splitting our Dataset into trainning and testing sets 80/20\n",
    "def Splite_clean_data(csv_file, colX, colY):\n",
    "    \n",
    "    # reading Clean Dataset\n",
    "    df = cleanText(csv_file)\n",
    "    \n",
    "    # Applying Clean function to remove unwanted characters , stopwords and apply STEMMING\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, colX] = stem_tokenize(df.loc[i,colX])\n",
    "\n",
    "    # Splitting dataset into trainning and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[colX], df[colY], test_size=0.2, random_state=7)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(Model, title):\n",
    "\n",
    "    # Spiltting the dataset after calling Clean function to pre-process datat before extracting feature\n",
    "    xtrain, xtest, ytrain, ytest = Splite_clean_data('train.csv', 'text', 'label')\n",
    "\n",
    "    # Initialization TF-IDF vector model to convert all textual content to numercial one\n",
    "    vector = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "    train_vector = vector.fit_transform(xtrain)\n",
    "    test_vector  = vector.transform(xtest)\n",
    "    \n",
    "    TF_IDF_model     = Model\n",
    "    TF_IDF_model.fit(train_vector, ytrain)\n",
    "    test_pred = TF_IDF_model.predict(test_vector)\n",
    "    \n",
    "    # Calculating accuracy score for trainning model\n",
    "    accuracy  = TF_IDF_model.score(train_vector, ytrain)*100\n",
    "    y_pred = TF_IDF_model.predict(test_vector)\n",
    "    \n",
    "    # Calculating accuracy score for testing model\n",
    "    acc_score = accuracy_score(ytest, y_pred)*100\n",
    "    class_report = classification_report(ytest, y_pred, output_dict=True)\n",
    "    class_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "    # Calculating f1_score for evalution our model\n",
    "    test_f1score = f1_score(ytest, y_pred)*100\n",
    "    \n",
    "    # plotting Confusin Matrix \n",
    "    skplt.metrics.plot_confusion_matrix(ytest, y_pred)\n",
    "    \n",
    "    print(title), print('*'*len(title))\n",
    "    print('Accuracy score train set :'+ format(accuracy, '.2f') + \"%\")\n",
    "    print('Accuracy score test set  :'+ format(acc_score, '.2f') + \"%\",'\\n')\n",
    "    print('F1 score:'+ format(test_f1score, '.2f') + \"%\",'\\n'), print('*'*len(title))\n",
    "    print('Classification Report: ')\n",
    "    print(class_df, '\\n'), print('*'*len(title))\n",
    "    plt.show()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Vector(Model, title, n):\n",
    "    \n",
    "    # Spiltting the dataset after calling Clean function to pre-process datat before extracting feature\n",
    "    xtrain, xtest, ytrain, ytest = Splite_clean_data('train.csv', 'text', 'label')\n",
    "\n",
    "    # Initialization Count Vectorizer vector model to convert all textual content to numercial one\n",
    "    vector = CountVectorizer(max_features=1000 , ngram_range=(n,n))\n",
    "    train_vector = vector.fit_transform(xtrain)\n",
    "    test_vector = vector.transform(xtest)\n",
    "    \n",
    "    count_vector_model     = Model\n",
    "    count_vector_model.fit(train_vector, ytrain)\n",
    "    y_pred = count_vector_model.predict(test_vector)\n",
    "    \n",
    "    # Calculating accuracy score for trainning model\n",
    "    accuracy  = count_vector_model.score(train_vector, ytrain)*100\n",
    "    y_pred = count_vector_model.predict(test_vector)\n",
    "    \n",
    "    # Calculating accuracy score for testing model\n",
    "    acc_score = accuracy_score(ytest, y_pred)*100\n",
    "    class_report = classification_report(ytest, y_pred, output_dict=True)\n",
    "    class_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "    # Calculating f1_score for evalution our model\n",
    "    test_f1score = f1_score(ytest, y_pred)*100\n",
    "    \n",
    "    # plotting Confusin Matrix \n",
    "    skplt.metrics.plot_confusion_matrix(ytest, y_pred)\n",
    "    \n",
    "    print(\"Models with \" , n , \"-grams :\\n\")\n",
    "    print('********************** \\n')\n",
    "    print(title), print('*'*len(title))\n",
    "    print('Accuracy score train set : '+ format(accuracy, '.2f') + \"%\")\n",
    "    print('Accuracy score test set  : '+ format(acc_score, '.2f') + \"%\",'\\n')\n",
    "    print('F1 score : '+ format(test_f1score, '.2f') + \"%\",'\\n'), print('*'*len(title))\n",
    "    print('Classification Report: ')\n",
    "    print(class_df, '\\n'), print('*'*len(title))\n",
    "    plt.show()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_PA_Model():\n",
    "    # Logistic Regression Classifier with TF_IDF\n",
    "    PA_Model = TF_IDF(Model = LogisticRegression(), \n",
    "                      title='TFIDF Logistic Regression Model : \\n')\n",
    "    return PA_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Vect_PA_Model():\n",
    "    # Logistic Regression  Classifier with count vectorizer\n",
    "    PA_Model = Count_Vector(Model = LogisticRegression(), \n",
    "                            title='ount verctorizer Passive Aggressive Model : \\n',\n",
    "                            n=2)\n",
    "    return PA_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Vect_RF():\n",
    "    # Random Forest Classifier with Count Vectorizer\n",
    "    RF_Model = Count_Vector(Model=RandomForestClassifier(), \n",
    "                            title='Count Vectorizer Random Forest Model : \\n ', \n",
    "                            n=2)\n",
    "    return RF_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_RF():\n",
    "    # Random Forest Classifier with TF_IDF\n",
    "    RF_Model = TF_IDF(Model=RandomForestClassifier(), \n",
    "                            title='TF-IDF Random Forest Model : \\n ')\n",
    "    return RF_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Vect_SVM():\n",
    "    # Random Forest Classifier with Count Vectorizer\n",
    "    SVM_Model = Count_Vector(Model=svm.LinearSVC(), \n",
    "                            title='Count Vectorizer Random Forest Model : \\n ', \n",
    "                            n=2)\n",
    "    return SVM_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_SVM():\n",
    "    # Random Forest Classifier with Count Vectorizer\n",
    "    SVM_Model = TF_IDF(Model=svm.LinearSVC(), \n",
    "                            title='Count Vectorizer Random Forest Model : \\n ')\n",
    "    return SVM_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF with Passive Aggressive classification Model \n",
    "if __name__ == '__main__':\n",
    "    TF_IDF_PA_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer with Passive Aggressive classification Model\n",
    "if __name__ == '__main__':\n",
    "    Count_Vect_PA_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF with Random Forest classification Model \n",
    "if __name__ == '__main__':\n",
    "    TF_IDF_RF_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer with Random Forest classification Model\n",
    "if __name__ == '__main__':\n",
    "    Count_Vect_RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer with SVM classification Model\n",
    "if __name__ == '__main__':\n",
    "    Count_Vect_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with SVM classification Model\n",
    "if __name__ == '__main__':\n",
    "    TF_IDF_SVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
